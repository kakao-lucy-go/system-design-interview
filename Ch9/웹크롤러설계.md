# 9장 웹 크롤러 설계

웹에 새로 올라오거나 갱신된 콘텐츠(웹 페이지, 비디오, 이미지, PDF 파일 등)를 찾아내는 기술. 몇 개의 웹 페이지에서 시작하여 링크를 따라 나가면서 새로운 콘텐츠를 수집.

- 검색 엔진 인덱싱: 크롤러를 가장 보편적으로 사용. 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스를 만든다.
- 웹 아카이빙: 웹에서 정보를 모아 장기보관한다.
- 웹 마이닝: 유용한 지식을 도출
- 웹 모니터링: 저작권이나 상표권이 침해되는 사례 모니터링

## 1단계: 문제 이해 및 설계 범위 확정

1. URL 집합이 입력으로 주어지면 URL 들이 가리키는 모든 웹 페이지를 다운로드한다.
2. 다운로드받은 웹 페이지에서 URL을 추출한다.
3. 추출된 URL을 다운로드할 URL 목록을 추가해서 위의 과정을 반복한다. 

### 설계 범위 좁히기

1. 크롤러의 용도 
2. 얼마나 많은 웹 페이지를 수집해야 하는지 
3. 새로 만들어진 웹, 수정된 웹도 고려해야 하는지
4. 수집한 웹 페이지를 얼만큼 보관해야 하는지
5. 중복된 콘텐츠는 어떻게 처리할지 

### 속성

1. 규모 확장성: 병행성(parallelism)을 활용
2. 안정성: 잘못 작성된 HTML, 무반응 서버, 장애, 악성코드 링크등에 대응 
3. 예절: 짧은 시간에 너무 많은 요청을 하지 않는다.
4. 확장성: 여러 형태에 대응할 수 있어야 한다.

### 개략적 규모 추정

- 매달 10억 개의 웹 페이지를 다운한다.
- QPS = 400페이지 / 초
- 최대(peak) QPS = 2 * 400 = 800QPS
- 웹 페이지 크기 평균 500k
- 월 500TB 저장 , 5년 30PB 저장

## 2단계 개략적 설계안 제시 및 동의 구하기

- 시작 URL 집합 : 출발점이기 때문에 시작 URL은 창의적일 필요가 있다. 가능한 많은 링크를 탐색할 수 있도록 한다. 보통 주제 별로 다른 시작 URL, 나라 별로 인기있는 URL 등을 시작점으로 한다.
- 미수집 URL 저장소 : 크롤링 상태를 다운로드할 URL, 다운로드된 URL로 나누어 관리하고, 다운로드할 URL 을 미수집 URL 저장소라 한다.
- HTML 다운로더: 웹 페이지를 다운로드하는 컴포넌트
- 도메인 이름 변환기: 웹 페이지 다운은 URL을 IP 주소로 바꿔야 한다. IP 주소를 알아낸다.(???)
- 콘텐츠 파서: 다운로드한 웹 페이지는 문제를 일으킬 수 있기 떄문에 파싱과 검증 절차를 거친다.
- 중복 콘텐츠: 웹 페이지의 해시 값을 비교해서 중복 콘텐츠 인지 확인한다.(???)
- 콘텐츠 저장소: HTML 문서를 보관. 디스크에 저장하고 인기 있는건 메모리에 둔다.
- URL 추출기: HTML 페이지를 파싱해서 링크를 골라낸다.
- URL 필터: 특정 파일 확장자를 갖는 URL, 오류 발생하는 URL, 접근 제외 목록에 포함된 URL 등을 필터링한다.
- 이미 방문한 URL: 볼륨 필터나 해시 테이블을 사용해 이미 방문한 URL이나 미수집 URL 저장소에 보관된 URL 을 추적한다.
- URL 저장소: 이미 방문한 URL 을 보관.

#### 순서 

1. 시작 URL 들을 미수집 URL 저장소에 저장
2. HTML 다운로더는 미수집 URL 저장소에서 URL 목록을 가져온다.
3. HTML 다운로더는 도메인 이름 변환기로 IP 주소를 알아내 다운받는다.
4. 콘텐츠 파서는 다운로드한 HTML을 검증한다.
5. 중복 콘텐츠 인지 확인
    1. 이미 저장됐으면 버린다.
    2. 저장안했으면 저장소에 저장한 뒤 URL 추출기로 전달한다.
6. URL 추출기는 링크를 골라낸다.
7. 골라낸 링크를 URL 필터로 전달한다.
8. 남은 URL을 중복 URL 여부를 판별한다.
9. 저장소에 없는 URL은 URL 저장소, 미수집 URL 저장소에 전달한다.(왜냐면 그 링크 HTML 안에 있는 링크를 또 찾기 위해)

## 3단계: 상세 설계

- DFS, BFS: 보통 웹 크롤링에서 DFS는 깊이가 가늠이 안되기 때문에 BFS를 사용한다.
    - 문제점:
        - 한 페이지에서 나오는 링크의 상당수는 같은 서버로 되돌아 간다. → 예의 없는 크롤러
        - URL 간에 우선순위가 없다.
- 미수집 URL 저장소 :
    - 예의 바른 크롤러는 한 번에 한 페이지만 요청한다.
        - 큐 라우터: 같은 호스트에 속한 URL 은 같은 큐에 들어가도록 한다.
        - 매핑 테이블: 호스트 이름과 큐 관계를 보관
        - FIFO: 같은 호스트에 속한 URL 은 항상 같은 큐에 보관한다.
        - 큐 선택기: 큐를 순회하면서 지정된 작업 스레드에 전달
        - 작업 스레드: 작업들 사이에 지연시간을 두고 순차적으로 처리
    
    우선순위: 페이지랭크, 트래픽 양, 갱신 빈도 등을 기준으로 순위를 결정해 우선순위 별로 큐가 하나씩 할당된다.
    
    - 순위 결정 장치: 우선순위 계산
    - 큐: 우선순위별로 큐 할당
    - 큐 선택기: 임의 큐에서 URL 을 꺼냄. 순위가 높은 큐에서 더 자주 꺼냄
    
    신선도: 다운로드한 페이지여도 주기적으로 재수집하다.
    
    - 변경 이력 활용
    - 우선순위 활용해서 중요한 페이지는 자주 재수집
    
    대부분의 URL은 디스크에 두고 메모리 버퍼에 큐를 둔다.
    
- HTML 다운로더: 페이지 다운
    - Robots.txt (로복 제외 프로토콜): 크롤러가 수집해도 되는 페이지 목록.
    - 성능 최적화
        - 분산 크롤링: 크롤링을 여러 서버에 분산.
        - 도메인 이름 변환 결과 캐시: DNS 호출은 동기적이기 때문에 블록된다. 따라서 DNS 결과로 얻어진 도메인 이름과 IP 주소와의 관계를 캐시에 보관해놓고 주기적으로 갱신한다.
        - 지역성: 크롤링 서버를 지역별로 분산
        - 짧은 타임아웃
    - 안정성:
        - 안정 해시: 다운로더 서버들에 부하를 분산할때 적용(5장)
        - 크롤링 상태 및 수집 데이터 저장: 크롤링 상태와 수집된 데이터를 지속적 저장장치에 기록해둔다.
        - 예외 처리
        - 데이터 검증
    - 확장성: 새로운 모듈을 끼워넣어 다른 형태의 콘텐츠도 다운할 수 있도록 한다.
    - 문제 있는 콘텐츠 감지 및 회피
        - 중복 콘텐츠: 해시나 체크섬으로 중복 콘텐츠를 탐지한다.(체크섬으로 어떻게?)
        - 거미 덫: 무한로프에 빠트리는 링크가 있는 경우 해결이 까다로워서 수작업을 발견한 덫을 필터 목록에 걸어둔다.
        - 데이터 노이즈: 가치 없는 콘텐츠는 필터링한다.

## 4단계: 마무리

추가 논의점

- 서버 측 렌더링
- 원치 않는 페이지 필터링
- 디비 다중화 및 샤딩
- 수평적 규모 확장성
- 가용성, 일관성, 안정성
- 데이터 분석 솔루션
